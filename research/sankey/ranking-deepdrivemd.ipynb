{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "479da07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import sys\n",
    "from pprint import pprint\n",
    "import argparse\n",
    "import os, glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "# plotting\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2ba5a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metric = 'frequency'\n",
    "#metric_ = metric + ' (avg)'\n",
    "\n",
    "#metric = 'frequency'\n",
    "#metric_ = metric + ' (sum)'\n",
    "\n",
    "#metric = 'access_size'\n",
    "#metric_ = metric + ' (avg)'\n",
    "\n",
    "#metric = 'access_size'\n",
    "#metric_ = metric + ' (sum)'\n",
    "\n",
    "metric = 'weighted_access_size'\n",
    "metric_ = metric + ' (sum)'\n",
    "\n",
    "filtered_file_ext = ['h5', 'pt', 'fits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "502f7379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f368e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2Dict(dlist, id_name):\n",
    "    return {x[id_name]: x for x in dlist}\n",
    "\n",
    "def import_yaml(fname):\n",
    "    with open(fname) as f:\n",
    "        return yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "539d710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversion(object):\n",
    "\n",
    "    data = None\n",
    "    job_dependency = None\n",
    "    tasks = None\n",
    "    cmds = None\n",
    "    files = None\n",
    "\n",
    "    def user_input(self):\n",
    "        parser = argparse.ArgumentParser('Pegasus DAX Parser')\n",
    "        parser.add_argument('-w', '--workflow_yml')\n",
    "        parser.add_argument('-r', '--replica_yml')\n",
    "        parser.add_argument('-t', '--transformation_yml')\n",
    "        parser.add_argument('-p', '--radical_pst_yml', help='output yaml filename')\n",
    "        parser.add_argument('-o', '--output', help='shell script output')\n",
    "        args = parser.parse_args()\n",
    "        self.args = args\n",
    "        return args\n",
    "\n",
    "\n",
    "    def import_dax(self):\n",
    "\n",
    "        data = None\n",
    "        jdep = None\n",
    "        tasks = None\n",
    "        files = None\n",
    "        cmds = None\n",
    "\n",
    "        if self.args.workflow_yml:\n",
    "            data = import_yaml(self.args.workflow_yml)\n",
    "            jdep = data['jobDependencies']\n",
    "\n",
    "            if 'transformationCatalog' in data:\n",
    "                tasks = convert2Dict(data['transformationCatalog']['transformations'], 'name')\n",
    "            if 'jobs' in data:\n",
    "                cmds = convert2Dict(data['jobs'], 'id')\n",
    "            if 'replicaCatalog' in data:\n",
    "                files = convert2Dict(data['replicaCatalog']['replicas'], 'lfn')\n",
    "        if self.args.replica_yml:\n",
    "            repl = import_yaml(self.args.replica_yml)\n",
    "            files = convert2Dict(repl['replicas'], 'lfn')\n",
    "        if self.args.transformation_yml:\n",
    "            trans = import_yaml(self.args.transformation_yml)\n",
    "            tasks = convert2Dict(trans['transformations'], 'name')\n",
    "\n",
    "        self.data = data\n",
    "        self.job_dependency = jdep\n",
    "        self.tasks = tasks\n",
    "        self.cmds = cmds\n",
    "        self.files = files\n",
    "\n",
    "\n",
    "    def get_jdep(self):\n",
    "        dict_dep = {}\n",
    "\n",
    "        for i in self.job_dependency:\n",
    "            dict_dep[i['id']] = i['children']\n",
    "\n",
    "        ordered = {}\n",
    "        for pjob_id, child_ids in dict_dep.items():\n",
    "            if pjob_id not in ordered:\n",
    "               ordered[pjob_id] = 1\n",
    "            for cjob_id in child_ids:\n",
    "                if cjob_id not in ordered:\n",
    "                   ordered[cjob_id] = ordered[pjob_id] + 1\n",
    "                else:\n",
    "                   ordered[cjob_id] = max(ordered[pjob_id] + 1, ordered[cjob_id])\n",
    "        self.ordered = ordered\n",
    "        self.ordered_by_val = {k: v for k, v in sorted(ordered.items(), key=lambda item: item[1])}\n",
    "\n",
    "\n",
    "    def get_cmd(self, cid):\n",
    "        cname = self.cmds[cid]['name']\n",
    "        #     {'name': 'mConvert', 'sites': [{'name': 'local', 'pfn':\n",
    "        #     '/files0/oddite/leeh736/Montage/bin/mConvert', 'type': 'stageable'}],\n",
    "        #     'profiles': {'condor': {'request_memory': '1 GB'}, 'env': {'PATH':\n",
    "        #     '/usr/bin:/bin:.'}}}\n",
    "        t_path = self.tasks[cname]['sites'][0]['pfn']\n",
    "        return t_path\n",
    "\n",
    "\n",
    "    def get_arguments(self, cid):\n",
    "        return self.cmds[cid]['arguments']\n",
    "\n",
    "\n",
    "    def get_data(self, cid):\n",
    "\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        pre_exec = []\n",
    "\n",
    "        flist = self.cmds[cid]['uses']\n",
    "        # uses: \n",
    "        # - lfn: pposs2ukstu_blue_001_002_area.fits\n",
    "        #   type: output\n",
    "        #   stageOut: false\n",
    "        #   registerReplica: true\n",
    "        # - lfn: poss2ukstu_blue_001_002.fits\n",
    "        #   type: input\n",
    "        for v in flist:\n",
    "            fname = v['lfn']\n",
    "            ftype = v['type']\n",
    "            # - lfn: region-oversized.hdr\n",
    "            #   pfns:\n",
    "            #   - site: local\n",
    "            #     pfn: file:///files0/oddite/leeh736/montage-workflow-v3/data/region-oversized.hdr\n",
    "            if fname in self.files:\n",
    "                fobj = self.files[fname]\n",
    "                flocation = fobj['pfns'][0]['site']\n",
    "                fpath = fobj['pfns'][0]['pfn']\n",
    "                if flocation == \"local\":\n",
    "                    # local copy to cwd\n",
    "                    if fpath[:7] == \"file://\":\n",
    "                        sp = 7\n",
    "                    else:\n",
    "                        sp = 0\n",
    "                    cmd = [\"/bin/cp\", fpath[sp:], \"./\"] # '7:', removing the `file://` protocol by indexing counts\n",
    "                else: # ipac\n",
    "                    # remote download to cwd\n",
    "                    cmd = [\"/bin/curl\", \"-o\", fname, f\"'{fpath}'\"]\n",
    "                pre_exec.append(cmd)\n",
    "            if ftype == \"output\":\n",
    "                outputs.append(fname)\n",
    "                if v['registerReplica']:\n",
    "                    self.files[fname] = {'pfns': [{'site': 'local', 'pfn': f'file://${cid}/{fname}'}]}\n",
    "            else:\n",
    "                inputs.append(fname)\n",
    "        return {'inputs': inputs,\n",
    "                'outputs': outputs,\n",
    "                'pre_exec': pre_exec}\n",
    "\n",
    "\n",
    "    def retrieve_cmd_info(self, cid):\n",
    "        t_exec = self.get_cmd(cid)\n",
    "        args = self.get_arguments(cid)\n",
    "        tmp = self.get_data(cid)\n",
    "        return {'exec': t_exec, 'args': args, 'inputs': tmp['inputs'], 'outputs':\n",
    "                tmp['outputs'], 'pre_exec': tmp['pre_exec']}\n",
    "\n",
    "\n",
    "    def update_id_map_to_pst(self, t_info, m_info):\n",
    "        pre_exec = t_info['pre_exec']\n",
    "        for cmd in pre_exec:\n",
    "            idx = 0\n",
    "            for element in cmd:\n",
    "                if element[0] == \"$\":\n",
    "                    idname, rest = element.split(\"/\",1)\n",
    "                    tmp = m_info[idname[1:]]\n",
    "                    cmd[idx] = \"%s/%s\" % (tmp, rest)\n",
    "                idx += 1\n",
    "\n",
    "    def convert_to_pst(self):\n",
    "        pst = {}\n",
    "        t_cnt = 0\n",
    "        id_map_to_pst = {}\n",
    "        for k, v in self.ordered_by_val.items():\n",
    "            t_info = self.retrieve_cmd_info(k)\n",
    "            s_name = 'stage_%s' % v\n",
    "            t_name = 'task_%s' % t_cnt\n",
    "            id_map_to_pst[k] = f'$pipeline_0_{s_name}_{t_name}'\n",
    "            self.update_id_map_to_pst(t_info, id_map_to_pst)\n",
    "            task = { t_name : t_info }\n",
    "            if s_name in pst:\n",
    "                pst[s_name].append(task)\n",
    "            else:\n",
    "                pst[s_name] = [task]\n",
    "            t_cnt += 1\n",
    "\n",
    "        pprint(pst, indent=4)\n",
    "        self.pst = pst\n",
    "\n",
    "    def save_pst_yml(self):\n",
    "        outfname = self.args.radical_pst_yml\n",
    "        if outfname:\n",
    "            with open(outfname, 'w') as outfile:\n",
    "                yaml.dump(self.pst, outfile, default_flow_style=False)\n",
    "\n",
    "    def save_shell_script(self):\n",
    "        outfname = self.args.output\n",
    "        if outfname:\n",
    "            with open(outfname, 'w') as outfile:\n",
    "                lines = []\n",
    "                for k, v in self.ordered_by_val.items():\n",
    "                    t_info = self.retrieve_cmd_info(k)\n",
    "                    for l in t_info['pre_exec']:\n",
    "                        outfile.write(\" \".join(l) + \"\\n\")\n",
    "                    line = \"%s %s \" % (t_info['exec'], \" \".join(t_info['args']))\n",
    "                    outfile.write(f\"{line}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11aa69dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args():\n",
    "    workflow_yml = None\n",
    "    replica_yml = None\n",
    "    transformation_yml = None\n",
    "\n",
    "args = args()\n",
    "args.workflow_yml = 'ddmd_workflow_360k.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99799dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Conversion()\n",
    "obj.args = args\n",
    "obj.import_dax()\n",
    "obj.get_jdep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9be2bc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading tazer stat files into pandas dataframe\n",
    "def stat_to_df(fname):\n",
    "\n",
    "    df = pd.read_csv(fname, sep=' ', names=['block_idx', 'frequency', 'access_size'], skiprows=1)\n",
    "    return df\n",
    "\n",
    "def read_tazer_stats(dpath):\n",
    "\n",
    "    agg_sum = {'frequency':'sum', 'access_size':'sum', 'weighted_access_size': 'sum'}\n",
    "    agg_avg = {'frequency':'mean', 'access_size':'mean', 'weighted_access_size': 'mean'}\n",
    "    agg_sum_rename = {'frequency': 'frequency (sum)', \n",
    "                      'access_size': 'access_size (sum)', \n",
    "                      'weighted_access_size': 'weighted_access_size (sum)'}\n",
    "    agg_avg_rename = {'frequency': 'frequency (avg)', \n",
    "                      'access_size': 'access_size (avg)',\n",
    "                      'weighted_access_size': 'weighted_access_size (avg)'}\n",
    "\n",
    "    df_all = {}\n",
    "\n",
    "    for fpath in glob.glob(dpath + \"/*/*_stat\"):\n",
    "\n",
    "        # ignore trace stats but for r/w stat\n",
    "        if fpath[-10:] == \"trace_stat\":\n",
    "            continue\n",
    "        df = stat_to_df(fpath)\n",
    "        if df.empty is True:\n",
    "            continue\n",
    "            \n",
    "        df['weighted_access_size'] = df['frequency'] * df['access_size']\n",
    "        task_name = os.path.basename(os.path.dirname(fpath))\n",
    "        stat_filename = os.path.basename(fpath)\n",
    "        series = df.agg(agg_sum)\n",
    "        df_sum = pd.DataFrame(series).transpose().rename(columns=agg_sum_rename)\n",
    "        series = df.agg(agg_avg)\n",
    "        df_avg = pd.DataFrame(series).transpose().rename(columns=agg_avg_rename)\n",
    "        df = pd.concat([df_sum, df_avg], axis=1)\n",
    "        if task_name in df_all:\n",
    "            if stat_filename in df_all[task_name]:\n",
    "                print(fname, \"==duplicate==\")\n",
    "            df_all[task_name][stat_filename] = df\n",
    "        else:\n",
    "            df_all[task_name] = {stat_filename: df}\n",
    "            \n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68c51ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom func for ddmd (non-pegasus)\n",
    "def parse_strace_stdout(strace_log_fname, filter_fname_ext):\n",
    "\n",
    "    fd_dict = {}\n",
    "    stat_dict = {}\n",
    "\n",
    "    with open(strace_log_fname) as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        fmode = \"\"\n",
    "        fperm = \"\"\n",
    "        fm = \"\"\n",
    "        func_name = line.split(\"(\")[0]\n",
    "        if func_name == \"open\":\n",
    "            func, ret = line.rsplit(\"=\", 1)\n",
    "            _, fname, __ = func.split('\"')\n",
    "            tmp = __.split(\",\")\n",
    "            if len(tmp) > 2:\n",
    "                fmode, fperm = tmp[1], tmp[2]\n",
    "            else:\n",
    "                fmode = tmp[1]\n",
    "            fmode = fmode.strip()\n",
    "            if \"O_RDONLY\" in fmode:\n",
    "                fm = \"r\"\n",
    "            else:\n",
    "                fm = \"w\"\n",
    "            ret = ret.strip()\n",
    "            try:\n",
    "                if int(ret) < 0:\n",
    "                    continue\n",
    "            except ValueError: # -1 ENOENT (No such file or directory)\n",
    "                continue\n",
    "            fname_ext = os.path.basename(fname).split(\".\")[-1]\n",
    "            #print(line, fname_ext)\n",
    "            if fname_ext not in filter_fname_ext:\n",
    "                continue\n",
    "            if ret not in fd_dict:\n",
    "                fd_dict[ret] = {'fname':fname, 'func_stat': func_name, \n",
    "                                'fmode': fm, 'fmode_ori': fmode, 'fperm': fperm,\n",
    "                               'stat_w':[], 'stat_r':[]}\n",
    "            else:\n",
    "                print(\"fd is duplicate!:\", ret, fname)\n",
    "            #print(fname, ret, fext, line)\n",
    "        elif func_name == \"read\" or func_name == \"pread64\":\n",
    "            func, ret = line.rsplit('=', 1)\n",
    "            tmp = func.split(\",\")\n",
    "            fd = tmp[0]\n",
    "            if len(tmp) >= 3:\n",
    "                msg, size, _ = tmp[1], tmp[2], tmp[3:]\n",
    "            fd = fd.split(\"(\")[1]\n",
    "            size = size.strip()[:-1] # removing \")\" by [:-1]\n",
    "            ret = ret.strip()\n",
    "            #print(fd, size, ret, line)\n",
    "            if fd not in fd_dict:\n",
    "                #print (\"%s is not in the list\" % fd)\n",
    "                continue\n",
    "            #print(line)\n",
    "            fd_dict[fd]['stat_r'].append(ret)\n",
    "            #print(        fd_dict[fd]['stat'])\n",
    "        elif func_name == \"write\" or func_name == \"pwrite64\":\n",
    "            func, ret = line.rsplit('=', 1)\n",
    "            tmp = func.split(\",\")\n",
    "            fd = tmp[0]\n",
    "            if len(tmp) >= 3:\n",
    "                msg, size, _ = tmp[1], tmp[2], tmp[3:]\n",
    "            fd = fd.split(\"(\")[1]\n",
    "            size = size.strip()[:-1] # removing \")\" by [:-1]\n",
    "            ret = ret.strip()\n",
    "            #print(fd, size, ret, line)\n",
    "            if fd not in fd_dict:\n",
    "                #print (\"%s is not in the list\" % fd)\n",
    "                continue\n",
    "            #print(line)\n",
    "            fd_dict[fd]['stat_w'].append(ret)\n",
    "            #print(        fd_dict[fd]['stat'])        \n",
    "        elif func_name == \"close\":\n",
    "            fd = line.split(\"(\")[1].split(\")\")[0]\n",
    "            #print(fd, line)\n",
    "            if fd not in fd_dict:\n",
    "                #print (fd, \" is not in the list\")\n",
    "                continue\n",
    "            fname = fd_dict[fd]['fname']\n",
    "            if fname in stat_dict:\n",
    "                #print (\"already exist!\", fname)\n",
    "                stat_dict[fname]['stat_w'] += fd_dict[fd]['stat_w']\n",
    "                stat_dict[fname]['stat_r'] += fd_dict[fd]['stat_r']            \n",
    "            else:\n",
    "                stat_dict[fname] = fd_dict[fd]\n",
    "            #print(\"deleting:\", fd, fd_dict[fd])\n",
    "            del(fd_dict[fd])\n",
    "            #break\n",
    "    return stat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80406bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddmd_stat_train = parse_strace_stdout('tazer_stat/ddmd/strace_train_no_tazer_on_gpu_qfs.2nd.log', filter_fname_ext=[\"h5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57c6aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddmd_stat_agent = parse_strace_stdout('tazer_stat/ddmd/strace_agent_no_tazer.qfs.without_virtual_h5_train_models.log', filter_fname_ext=[\"h5\",\"pt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4858b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddmd_stat_agg = parse_strace_stdout('tazer_stat/ddmd/strace_agg_output.rand.log', filter_fname_ext=[\"h5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8678da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df_from_strace(stat_dict, task_name):\n",
    "\n",
    "    df_all = {task_name:{}}\n",
    "    for k, v in stat_dict.items():\n",
    "        #print(k,v)\n",
    "        for ftype in [\"r\", \"w\"]:\n",
    "            stat_filename = os.path.basename(k) + \"_%s_stat\" % ftype\n",
    "            df = pd.DataFrame(stat_dict[k][f'stat_{ftype}'], columns=['access_size'])\n",
    "            df = df.astype({'access_size':'int'})\n",
    "            df['frequency'] = 1\n",
    "            df['block_idx'] = df.index\n",
    "\n",
    "            agg_sum = {'frequency':'sum', 'access_size':'sum', 'weighted_access_size': 'sum'}\n",
    "            agg_avg = {'frequency':'mean', 'access_size':'mean', 'weighted_access_size': 'mean'}\n",
    "            agg_sum_rename = {'frequency': 'frequency (sum)', \n",
    "                              'access_size': 'access_size (sum)', \n",
    "                              'weighted_access_size': 'weighted_access_size (sum)'}\n",
    "            agg_avg_rename = {'frequency': 'frequency (avg)', \n",
    "                              'access_size': 'access_size (avg)',\n",
    "                              'weighted_access_size': 'weighted_access_size (avg)'}\n",
    "\n",
    "            if df.empty is True:\n",
    "                continue\n",
    "            #print(df.dtypes)\n",
    "\n",
    "            df['weighted_access_size'] = df['frequency'] * df['access_size']\n",
    "            series = df.agg(agg_sum)\n",
    "            df_sum = pd.DataFrame(series).transpose().rename(columns=agg_sum_rename)\n",
    "            series = df.agg(agg_avg)\n",
    "            df_avg = pd.DataFrame(series).transpose().rename(columns=agg_avg_rename)\n",
    "            df = pd.concat([df_sum, df_avg], axis=1)\n",
    "            \n",
    "            if stat_filename in df_all[task_name]:\n",
    "                df_prev = df_all[task_name][stat_filename]\n",
    "                #tmp = pd.concat([df, df_prev],axis=0, ignore_index=True)\n",
    "                df = pd.concat([df, df_prev],axis=0).groupby(level=0).mean()\n",
    "                #tmp['block_idx'] = tmp.index\n",
    "                #df = tmp\n",
    "                #print(df)            \n",
    "            df_all[task_name][stat_filename] = df\n",
    "            #print(task_name, stat_filename)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7355b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_train = build_df_from_strace(ddmd_stat_train, task_name=\"train_py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e027a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_agent = build_df_from_strace(ddmd_stat_agent, task_name=\"lof_py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31b9ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_agg = build_df_from_strace(ddmd_stat_agg, task_name=\"aggregate_py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "012b945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = {**df_all_train, **df_all_agent, **df_all_agg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2a7775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(data, df, metric_=metric_):\n",
    "    print(\"Building a graph with this metric:\", metric_)\n",
    "    _pos = (0, -1)\n",
    "    G = nx.DiGraph()\n",
    "    prev_v = 1\n",
    "    cnt = 0\n",
    "    for k, v in data.ordered_by_val.items():\n",
    "        if prev_v == v:\n",
    "            cnt += 1\n",
    "        else:\n",
    "            cnt = 0\n",
    "        if cnt > 10000:\n",
    "            prev_v = v\n",
    "            continue\n",
    "        t_info = data.retrieve_cmd_info(k)\n",
    "        tname = os.path.basename(t_info['exec'])\n",
    "        tnodename = \"%s (%s)\" % (tname, k)\n",
    "        if prev_v == v:\n",
    "            _pos = (_pos[0], _pos[1] + 1)\n",
    "        else:\n",
    "            _pos = (v + 2, 0)\n",
    "        G.add_node(tnodename, ntype='task', pos=_pos)\n",
    "        #rint(tnodename)\n",
    "        prev_v = v\n",
    "        __pos = _pos\n",
    "        for ftype in ['inputs', 'outputs']:\n",
    "\n",
    "            bnames = [x for x in t_info[ftype] if x.split('.')[1] in filtered_file_ext]\n",
    "            __pos = (_pos[0] + 2.5, __pos[1])\n",
    "            for bname in bnames:\n",
    "                sname = bname + \"_r_stat\" if ftype == 'inputs' else bname + \"_w_stat\"\n",
    "                if tname not in df:\n",
    "                    print (tname, \"--missing--\")\n",
    "                    continue\n",
    "                if sname not in df[tname]:\n",
    "                    print(tname, sname , \"==missing==\")\n",
    "                    continue\n",
    "                stat = df[tname][sname]\n",
    "                frequency = stat[metric_][0]\n",
    "                frequency_sum = stat['frequency (sum)'][0]\n",
    "                if G.has_node(bname) is False:\n",
    "                    G.add_node(bname, pos=__pos)\n",
    "                    __pos = (__pos[0], __pos[1] + 1)\n",
    "                if ftype == 'inputs':\n",
    "                    G.add_edge(bname, tnodename, value=frequency, frequency_sum=frequency_sum)\n",
    "                else:\n",
    "                    G.add_edge(tnodename, bname, value=frequency, frequency_sum=frequency_sum)\n",
    "        _pos = (_pos[0], __pos[1])\n",
    "        #_pos = (_pos[0] + 1, 0) \n",
    "    return G, _pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abccf86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_md = read_tazer_stats('tazer_stat/ddmd/tazer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c14528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_ = { **df_md, **df_all}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "787d1525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a graph with this metric: weighted_access_size (sum)\n",
      "Building a graph with this metric: frequency (avg)\n"
     ]
    }
   ],
   "source": [
    "#df_all = read_tazer_stats(\"tazer_stat/montage\")\n",
    "G, _pos = get_graph(obj, df_all_)\n",
    "metric_frequency_avg = 'frequency (avg)'\n",
    "G_, __pos = get_graph(obj, df_all_, metric_=metric_frequency_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "882e39d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('run_openmm_py (ID0000002)', 'stage0000_task0001.h5'),\n",
       " ('stage0000_task0001.h5', 'aggregate_py (ID0000013)'),\n",
       " ('aggregate_py (ID0000013)', 'aggregated.h5'),\n",
       " ('aggregated.h5', 'train_py (ID0000014)'),\n",
       " ('train_py (ID0000014)', 'embeddings-epoch-1-20230112-125118.h5')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def critical_path_edges(G, weight):\n",
    "    path = nx.dag_longest_path(G, weight)\n",
    "    path_edges = list(zip(path,path[1:]))\n",
    "    return path_edges\n",
    "\n",
    "cp_edges = critical_path_edges(G, \"value\")\n",
    "cp_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b245402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fan_in_edges(G, cutoff=5):\n",
    "    fan_in_edges = []\n",
    "    for n, attr in G.nodes(data=True):\n",
    "        if  'ntype' in attr and attr['ntype'] == 'task':\n",
    "            fan_in_nodes = [x for x in G.predecessors(n)]\n",
    "            if len(fan_in_nodes) > cutoff:\n",
    "                fan_in_edges += [(x, n) for x in fan_in_nodes]\n",
    "    return fan_in_edges\n",
    "\n",
    "def fan_out_edges(G, cutoff=5):\n",
    "    fan_out_edges = []\n",
    "    for n, attr in G.nodes(data=True):\n",
    "        if  'ntype' in attr and attr['ntype'] == 'task':\n",
    "            fan_out_nodes = [x for x in G.successors(n)]\n",
    "            if len(fan_out_nodes) > cutoff:\n",
    "                fan_out_edges += [(n, x) for x in fan_out_nodes]\n",
    "    return fan_out_edges\n",
    "\n",
    "fin_edges = fan_in_edges(G)\n",
    "fout_edges = fan_out_edges(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70606c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_compressed_fan_in(G, fan_in_edges, threshold=.8):\n",
    "    incoming_vol_size=0\n",
    "    outgoing_vol_size=0\n",
    "    for u, v in fan_in_edges:\n",
    "        attr = G.get_edge_data(u, v)\n",
    "        if 'frequency_sum' in attr:\n",
    "            incoming_vol_size += attr['frequency_sum']\n",
    "\n",
    "    for n in G.successors(v):\n",
    "        attr = G.get_edge_data(v, n)\n",
    "        if 'frequency_sum' in attr:\n",
    "            outgoing_vol_size += attr['frequency_sum']\n",
    "        \n",
    "    return (outgoing_vol_size/incoming_vol_size) <= threshold\n",
    "\n",
    "is_compressed_fan_in(G, fin_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b674e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node count:39\n",
      "edge count:38\n"
     ]
    }
   ],
   "source": [
    "print (\"node count:%s\" % len(G.nodes()))\n",
    "print (\"edge count:%s\" % len(G.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58ca736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_for_producer_consumer(G, unit=\"byte\"):\n",
    "    \n",
    "    is_round = True\n",
    "    \n",
    "    column1 = []\n",
    "    column2 = []\n",
    "    #column3 = []\n",
    "    #column4 = []\n",
    "    if unit.lower() == \"gb\":\n",
    "        unit_size = 10e8\n",
    "    elif unit.lower() == \"mb\":\n",
    "        unit_size = 10e5\n",
    "    elif unit.lower() == \"kb\":\n",
    "        unit_size = 10e2\n",
    "    else:\n",
    "        unit = \"byte\"\n",
    "        unit_size = 10e-1\n",
    "        \n",
    "    for node in G.nodes():\n",
    "        # skip if not a file vertex where a task (producer) - a file and a task (consumer) could form\n",
    "        if len(node.split('.')) <= 1:\n",
    "            continue\n",
    "        for iedge in G.in_edges(node):\n",
    "            for oedge in G.out_edges(node):\n",
    "                ival = G.edges[iedge]['value']\n",
    "                oval = G.edges[oedge]['value']\n",
    "                column1.append((iedge, oedge))\n",
    "                column2.append(round((ival + oval) / unit_size, 2))\n",
    "                #column3.append(ival)\n",
    "                #column4.append(oval)\n",
    "    return pd.DataFrame({'task (p) - file, file - task (c) edges':column1, \n",
    "                         f'access_size ({unit})': column2})\n",
    "    \n",
    "def is_node_type(node_name, ntype_expected):\n",
    "    \n",
    "    if len(node_name.split('.')) <= 1:\n",
    "        ntype_found = \"task\"\n",
    "    else:\n",
    "        ntype_found = \"file\"\n",
    "    return ntype_found == ntype_expected\n",
    "\n",
    "def _table_for_metric(G, element_type, edges_to_collect=\"out\", unit=\"byte\"):\n",
    "    \n",
    "    is_round = True\n",
    "    \n",
    "    if element_type == \"producer-consumer\":\n",
    "        node_selection = 'file'\n",
    "    elif element_type == \"file\":\n",
    "        node_selection = 'file'\n",
    "    elif element_type == \"task\":\n",
    "        node_selection = 'task'\n",
    "    \n",
    "    col1 = []\n",
    "    col2 = []\n",
    "    #col3 = []\n",
    "    #col4 = []\n",
    "    if unit.lower() == \"gb\":\n",
    "        unit_size = 10e8\n",
    "    elif unit.lower() == \"mb\":\n",
    "        unit_size = 10e5\n",
    "    elif unit.lower() == \"kb\":\n",
    "        unit_size = 10e2\n",
    "    else:\n",
    "        unit = \"byte\"\n",
    "        unit_size = 10e-1\n",
    "        \n",
    "    for node in G.nodes():\n",
    "        # skip if not a file vertex where a task (producer) - a file and a task (consumer) could form\n",
    "        if not is_node_type(node, node_selection):\n",
    "            continue\n",
    "        \n",
    "        if element_type == \"file\" or element_type == \"task\":\n",
    "            val = 0\n",
    "            if edges_to_collect == \"out\" or edges_to_collect == \"both\":\n",
    "                for edge in G.out_edges(node):\n",
    "                    val += G.edges[edge]['value']\n",
    "            if edges_to_collect == \"in\" or edges_to_collect == \"both\":\n",
    "                for edge in G.in_edges(node):\n",
    "                    val += G.edges[edge]['value']\n",
    "            col1.append(node)\n",
    "            col2.append(round(val / unit_size, 2) if is_round else val)\n",
    "        else: # \"producer-consumer\"\n",
    "            for iedge in G.in_edges(node):\n",
    "                for oedge in G.out_edges(node):\n",
    "                    ival = G.edges[iedge]['value']\n",
    "                    oval = G.edges[oedge]['value']\n",
    "                    col1.append((iedge, oedge))\n",
    "                    col2.append(round((ival + oval) / unit_size, 2)  if is_round else val)\n",
    "                    #col3.append(ival)\n",
    "                    #col4.append(oval)\n",
    "    return col1, col2\n",
    "                \n",
    "\n",
    "def table_for_file_node(G, edges_to_collect=\"out\", unit=\"byte\"):\n",
    "    node_type = 'file'\n",
    "    col1, col2 = _table_for_metric(G, node_type, edges_to_collect=edges_to_collect, unit=unit)\n",
    "    return pd.DataFrame({f'node ({node_type})':col1, \n",
    "                         f'access_size ({unit})': col2})\n",
    "\n",
    "\n",
    "def table_for_task_node(G, edges_to_collect=\"in\", unit=\"byte\"):\n",
    "    node_type = 'task'\n",
    "    col1, col2 = _table_for_metric(G, node_type, edges_to_collect=edges_to_collect, unit=unit)\n",
    "    return pd.DataFrame({f'node ({node_type})':col1, \n",
    "                         f'access_size ({unit})': col2})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c22cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_unit = \"GB\"\n",
    "df_pc = table_for_producer_consumer(G, unit=my_unit) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d643f979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Table for Task-File-Task (Sorted by access_size (GB))**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task (p) - file, file - task (c) edges</th>\n",
       "      <th>access_size (GB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>((aggregate_py (ID0000013), aggregated.h5), (aggregated.h5, train_py (ID0000014)))</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>((aggregate_py (ID0000013), aggregated.h5), (aggregated.h5, lof_py (ID0000015)))</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>((run_openmm_py (ID0000001), stage0000_task0000.h5), (stage0000_task0000.h5, aggregate_py (ID0000013)))</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((run_openmm_py (ID0000002), stage0000_task0001.h5), (stage0000_task0001.h5, aggregate_py (ID0000013)))</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>((run_openmm_py (ID0000003), stage0000_task0002.h5), (stage0000_task0002.h5, aggregate_py (ID0000013)))</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>((run_openmm_py (ID0000005), stage0000_task0004.h5), (stage0000_task0004.h5, aggregate_py (ID0000013)))</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>((run_openmm_py (ID0000006), stage0000_task0005.h5), (stage0000_task0005.h5, aggregate_py (ID0000013)))</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>((run_openmm_py (ID0000007), stage0000_task0006.h5), (stage0000_task0006.h5, aggregate_py (ID0000013)))</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>((run_openmm_py (ID0000008), stage0000_task0007.h5), (stage0000_task0007.h5, aggregate_py (ID0000013)))</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>((run_openmm_py (ID0000009), stage0000_task0008.h5), (stage0000_task0008.h5, aggregate_py (ID0000013)))</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>((run_openmm_py (ID0000010), stage0000_task0009.h5), (stage0000_task0009.h5, aggregate_py (ID0000013)))</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>((run_openmm_py (ID0000011), stage0000_task0010.h5), (stage0000_task0010.h5, aggregate_py (ID0000013)))</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>((run_openmm_py (ID0000004), stage0000_task0003.h5), (stage0000_task0003.h5, aggregate_py (ID0000013)))</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>((run_openmm_py (ID0000012), stage0000_task0011.h5), (stage0000_task0011.h5, aggregate_py (ID0000013)))</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     task (p) - file, file - task (c) edges  \\\n",
       "12                       ((aggregate_py (ID0000013), aggregated.h5), (aggregated.h5, train_py (ID0000014)))   \n",
       "13                         ((aggregate_py (ID0000013), aggregated.h5), (aggregated.h5, lof_py (ID0000015)))   \n",
       "0   ((run_openmm_py (ID0000001), stage0000_task0000.h5), (stage0000_task0000.h5, aggregate_py (ID0000013)))   \n",
       "1   ((run_openmm_py (ID0000002), stage0000_task0001.h5), (stage0000_task0001.h5, aggregate_py (ID0000013)))   \n",
       "2   ((run_openmm_py (ID0000003), stage0000_task0002.h5), (stage0000_task0002.h5, aggregate_py (ID0000013)))   \n",
       "4   ((run_openmm_py (ID0000005), stage0000_task0004.h5), (stage0000_task0004.h5, aggregate_py (ID0000013)))   \n",
       "5   ((run_openmm_py (ID0000006), stage0000_task0005.h5), (stage0000_task0005.h5, aggregate_py (ID0000013)))   \n",
       "6   ((run_openmm_py (ID0000007), stage0000_task0006.h5), (stage0000_task0006.h5, aggregate_py (ID0000013)))   \n",
       "7   ((run_openmm_py (ID0000008), stage0000_task0007.h5), (stage0000_task0007.h5, aggregate_py (ID0000013)))   \n",
       "8   ((run_openmm_py (ID0000009), stage0000_task0008.h5), (stage0000_task0008.h5, aggregate_py (ID0000013)))   \n",
       "9   ((run_openmm_py (ID0000010), stage0000_task0009.h5), (stage0000_task0009.h5, aggregate_py (ID0000013)))   \n",
       "10  ((run_openmm_py (ID0000011), stage0000_task0010.h5), (stage0000_task0010.h5, aggregate_py (ID0000013)))   \n",
       "3   ((run_openmm_py (ID0000004), stage0000_task0003.h5), (stage0000_task0003.h5, aggregate_py (ID0000013)))   \n",
       "11  ((run_openmm_py (ID0000012), stage0000_task0011.h5), (stage0000_task0011.h5, aggregate_py (ID0000013)))   \n",
       "\n",
       "    access_size (GB)  \n",
       "12              2.40  \n",
       "13              0.88  \n",
       "0               0.05  \n",
       "1               0.05  \n",
       "2               0.05  \n",
       "4               0.05  \n",
       "5               0.05  \n",
       "6               0.05  \n",
       "7               0.05  \n",
       "8               0.05  \n",
       "9               0.05  \n",
       "10              0.05  \n",
       "3               0.04  \n",
       "11              0.04  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', False) # 500)\n",
    "\n",
    "curr_metric = f'access_size ({my_unit})'\n",
    "print()\n",
    "printmd(f\"**Table for Task-File-Task (Sorted by {curr_metric})**\")\n",
    "df_pc_sorted = df_pc.sort_values(by=f'access_size ({my_unit})', ascending=False)\n",
    "df_pc_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ff14f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.86"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pc['access_size (GB)'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe1474e",
   "metadata": {},
   "source": [
    "### Table for Tasks (Sorted by Access Size-Sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64beb582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node (task)</th>\n",
       "      <th>access_size (GB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>train_py (ID0000014)</td>\n",
       "      <td>1.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>aggregate_py (ID0000013)</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lof_py (ID0000015)</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>run_openmm_py (ID0000001)</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>run_openmm_py (ID0000002)</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>run_openmm_py (ID0000003)</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>run_openmm_py (ID0000004)</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>run_openmm_py (ID0000005)</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>run_openmm_py (ID0000006)</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>run_openmm_py (ID0000007)</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>run_openmm_py (ID0000008)</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>run_openmm_py (ID0000009)</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>run_openmm_py (ID0000010)</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>run_openmm_py (ID0000011)</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>run_openmm_py (ID0000012)</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  node (task)  access_size (GB)\n",
       "13       train_py (ID0000014)              1.90\n",
       "12   aggregate_py (ID0000013)              0.54\n",
       "14         lof_py (ID0000015)              0.41\n",
       "0   run_openmm_py (ID0000001)              0.00\n",
       "1   run_openmm_py (ID0000002)              0.00\n",
       "2   run_openmm_py (ID0000003)              0.00\n",
       "3   run_openmm_py (ID0000004)              0.00\n",
       "4   run_openmm_py (ID0000005)              0.00\n",
       "5   run_openmm_py (ID0000006)              0.00\n",
       "6   run_openmm_py (ID0000007)              0.00\n",
       "7   run_openmm_py (ID0000008)              0.00\n",
       "8   run_openmm_py (ID0000009)              0.00\n",
       "9   run_openmm_py (ID0000010)              0.00\n",
       "10  run_openmm_py (ID0000011)              0.00\n",
       "11  run_openmm_py (ID0000012)              0.00"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_task = table_for_task_node(G, unit=my_unit)\n",
    "df_task.sort_values(by=f'access_size ({my_unit})', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a73703c",
   "metadata": {},
   "source": [
    "### Table for Input/Output Files (Sorted by Access Size-Sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "527eaad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Table for Input/Output Files (Sorted by access_size (GB))**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node (file)</th>\n",
       "      <th>access_size (GB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>aggregated.h5</td>\n",
       "      <td>2.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stage0000_task0007.h5</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stage0000_task0001.h5</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stage0000_task0010.h5</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stage0000_task0009.h5</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stage0000_task0008.h5</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stage0000_task0000.h5</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stage0000_task0006.h5</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stage0000_task0005.h5</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>embeddings-epoch-1-20230112-125118.h5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>embeddings-epoch-4-20230112-125526.h5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>embeddings-epoch-5-20230112-125655.h5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>embeddings-epoch-6-20230112-131442.h5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>embeddings-epoch-7-20230112-131601.h5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>embeddings-epoch-8-20230112-131719.h5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>embeddings-epoch-9-20230112-131848.h5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>embeddings-epoch-10-20230112-132011.h5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>embeddings-epoch-3-20230112-125357.h5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               node (file)  access_size (GB)\n",
       "12                           aggregated.h5              2.28\n",
       "7                    stage0000_task0007.h5              0.05\n",
       "1                    stage0000_task0001.h5              0.05\n",
       "10                   stage0000_task0010.h5              0.05\n",
       "9                    stage0000_task0009.h5              0.05\n",
       "8                    stage0000_task0008.h5              0.05\n",
       "0                    stage0000_task0000.h5              0.05\n",
       "6                    stage0000_task0006.h5              0.05\n",
       "5                    stage0000_task0005.h5              0.05\n",
       "..                                     ...               ...\n",
       "13   embeddings-epoch-1-20230112-125118.h5              0.00\n",
       "16   embeddings-epoch-4-20230112-125526.h5              0.00\n",
       "17   embeddings-epoch-5-20230112-125655.h5              0.00\n",
       "18   embeddings-epoch-6-20230112-131442.h5              0.00\n",
       "19   embeddings-epoch-7-20230112-131601.h5              0.00\n",
       "20   embeddings-epoch-8-20230112-131719.h5              0.00\n",
       "21   embeddings-epoch-9-20230112-131848.h5              0.00\n",
       "22  embeddings-epoch-10-20230112-132011.h5              0.00\n",
       "15   embeddings-epoch-3-20230112-125357.h5              0.00\n",
       "\n",
       "[24 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_file = table_for_file_node(G, unit=my_unit)\n",
    "curr_metric = f'access_size ({my_unit})'\n",
    "print()\n",
    "printmd(f\"**Table for Input/Output Files (Sorted by {curr_metric})**\")\n",
    "df_file.sort_values(by=curr_metric, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1771ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>aggregated.h5</td>\n",
       "      <td>train_py (ID0000014)</td>\n",
       "      <td>1898819204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>aggregate_py (ID0000013)</td>\n",
       "      <td>aggregated.h5</td>\n",
       "      <td>503656827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>aggregated.h5</td>\n",
       "      <td>lof_py (ID0000015)</td>\n",
       "      <td>378731152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stage0000_task0001.h5</td>\n",
       "      <td>aggregate_py (ID0000013)</td>\n",
       "      <td>45756056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>stage0000_task0010.h5</td>\n",
       "      <td>aggregate_py (ID0000013)</td>\n",
       "      <td>45502104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stage0000_task0002.h5</td>\n",
       "      <td>aggregate_py (ID0000013)</td>\n",
       "      <td>45342360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source                    target       value\n",
       "25             aggregated.h5      train_py (ID0000014)  1898819204\n",
       "24  aggregate_py (ID0000013)             aggregated.h5   503656827\n",
       "26             aggregated.h5        lof_py (ID0000015)   378731152\n",
       "3      stage0000_task0001.h5  aggregate_py (ID0000013)    45756056\n",
       "21     stage0000_task0010.h5  aggregate_py (ID0000013)    45502104\n",
       "5      stage0000_task0002.h5  aggregate_py (ID0000013)    45342360"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = nx.to_pandas_edgelist(G).sort_values('value', ascending=False)[:6]\n",
    "df[['source', 'target', 'value']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
